{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ed2dc-5fc9-44c2-8d63-c943d619de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from functools import reduce\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840bab59-4428-44a9-b0f0-01ef8457b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a7f382-248a-4570-ad40-076e815e9edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(ds1, ds2):\n",
    "    \n",
    "    \"\"\" Merges two datasets based on MRN and Discharge Date. \n",
    "    Integrates in order ds1, ds2.\n",
    "    For the same column, ds2 will only fill in ds1 blanks.\n",
    "    \"\"\"\n",
    "\n",
    "    merged_df = pd.merge(ds1, ds2, on=['MRN', 'Discharge_Date'], how='outer', suffixes=(\"_ds1\", \"_ds2\"))\n",
    "    merged_df[\"Admission_Date_ds1\"] = pd.to_datetime(merged_df[\"Admission_Date_ds1\"]).dt.date\n",
    "    merged_df[\"Admission_Date_ds2\"] = pd.to_datetime(merged_df[\"Admission_Date_ds2\"]).dt.date\n",
    "\n",
    "    common_columns = set(ds1.columns).intersection(ds2.columns)\n",
    "    cols_to_ignore = ['MRN', 'Discharge_Date', 'Admission_Date']\n",
    "    cols_to_fill = common_columns - set(cols_to_ignore)\n",
    "\n",
    "    for col in cols_to_fill:\n",
    "        merged_df[col + \"_ds1\"].fillna(merged_df[col + \"_ds2\"], inplace=True)\n",
    "        \n",
    "    merged_df['Admission_Date_1'] = merged_df.apply(lambda row:                                                         #### Catch missing and inconsistent admission dates\n",
    "                                              row['Admission_Date_ds1'] if pd.isnull(row['Admission_Date_ds2']) else \n",
    "                                              (row['Admission_Date_ds2'] if pd.isnull(row['Admission_Date_ds1']) else \n",
    "                                               min(row['Admission_Date_ds1'], row['Admission_Date_ds2'])), axis=1)      #### Use the earliest admission date\n",
    "    \n",
    "    merged_df.drop(columns=['Admission_Date_ds1', 'Admission_Date_ds2'], inplace=True)\n",
    "    merged_df.drop(columns=[col + '_ds2' for col in cols_to_fill], inplace=True)\n",
    "    merged_df.columns = [col.replace('_ds1', '') for col in merged_df.columns]\n",
    "    merged_df.sort_values(by=['MRN', 'Discharge_Date'], inplace=True)\n",
    "    merged_df.rename(columns={'Admission_Date_1': 'Admission_Date'}, inplace=True)\n",
    "    merged_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return merged_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9332a01a-6bf9-4028-b4a9-082d2f823afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_suppdata(ds, supp, name): \n",
    "    ''' Merging supplementary data: Weight, NT-proBNP, HbA1c\n",
    "    Keeps all entries within admission and discharge date and has 1 date and 1 value column per entry.\n",
    "    '''\n",
    "    \n",
    "    ds = ds.loc[:, ~ds.columns.str.contains('^Unnamed')]   \n",
    "    supp = supp.loc[:, ~supp.columns.str.contains('^Unnamed')]\n",
    "    \n",
    "    supp_2 = supp[supp['MRN'].isin(ds['MRN'])]   # Don't need to consider patients that are not in our dataset\n",
    "    \n",
    "    merged_addis = pd.merge(ds, supp_2, on=['MRN'], how=\"left\")\n",
    "    merged_addis = merged_addis.loc[:, ~merged_addis.columns.str.contains('^Unnamed')]\n",
    "    merged_addis.sort_values(by=['MRN', 'Discharge_Date'], inplace=True)\n",
    "    \n",
    "    # Admission and Discahrge test results, accept within 24 hours of admission/discharge rather than just the day\n",
    "    merged_addis[f'{name}_Admission'] = merged_addis.apply(lambda row: row['Test Result'] if (not pd.isnull(row['RequestDate']) and (not pd.isnull(row['Admission_Date'])) and\n",
    "                                                                                             (((row['Admission_Date'] + pd.Timedelta(days=1)) == row['RequestDate']) or (row['Admission_Date'] == row['RequestDate']))) else np.nan, axis=1)\n",
    "\n",
    "    merged_addis[f'{name}_Discharge'] = merged_addis.apply(lambda row: row['Test Result'] if (not pd.isnull(row['RequestDate']) and (not pd.isnull(row['Discharge_Date'])) and\n",
    "                                                                                             (((row['Discharge_Date'] - pd.Timedelta(days=1)) == row['RequestDate']) or (row['Discharge_Date'] == row['RequestDate']))) else np.nan, axis=1)\n",
    "\n",
    "    # Just need the admission and discharge data\n",
    "    cols_to_drop = supp.columns.tolist()  \n",
    "    cols_to_drop.remove('MRN') \n",
    "\n",
    "    merged_addis.drop(columns=cols_to_drop, inplace=True)\n",
    "                   \n",
    "    addis_sup = pd.merge(merged_addis, supp_2, on=['MRN'], how=\"left\", suffixes=(\"_ds\", \"_supp\"))\n",
    "    \n",
    "    addis_sup.loc[:, 'RequestDate'] = pd.to_datetime(addis_sup['RequestDate'], format='%Y-%m-%d')\n",
    "    addis_sup.loc[:, 'Admission_Date'] = pd.to_datetime(addis_sup['Admission_Date'])\n",
    "    addis_sup.loc[:, 'Discharge_Date'] = pd.to_datetime(addis_sup['Discharge_Date'])\n",
    "    merged_addis.loc[:, 'Admission_Date'] = pd.to_datetime(merged_addis['Admission_Date'])\n",
    "    \n",
    "    # Only need the test results that are within the episode\n",
    "    filtered_df = addis_sup[\n",
    "    (addis_sup[\"RequestDate\"] >= addis_sup[\"Admission_Date\"]) &\n",
    "    (addis_sup[\"RequestDate\"] <= addis_sup['Discharge_Date'])\n",
    "    ]\n",
    "\n",
    "    grouped = filtered_df.groupby([\"MRN\", \"Discharge_Date\"])\n",
    "\n",
    "\n",
    "    # Get list of all test results and dates for MRNs within datasets for same discharge date - less inconsistent than admission\n",
    "    result_df = grouped.agg({\n",
    "        'RequestDate': list,\n",
    "        'Test Result': list\n",
    "    }).reset_index()\n",
    "\n",
    "    \n",
    "    result_df.loc[:, 'Discharge_Date'] = pd.to_datetime(result_df['Discharge_Date'])\n",
    "    \n",
    "\n",
    "    # Forward and back fill so can then drop duplicates - captures inconsistent dates and deals with multiple episodes\n",
    "    merged_addis[f'{name}_Admission'] = merged_addis.groupby(['MRN', 'Discharge_Date'])[f'{name}_Admission'].bfill()\n",
    "    merged_addis[f'{name}_Admission'] = merged_addis.groupby(['MRN', 'Discharge_Date'])[f'{name}_Admission'].ffill()\n",
    "\n",
    "    merged_addis[f'{name}_Discharge'] = merged_addis.groupby(['MRN', 'Discharge_Date'])[f'{name}_Discharge'].bfill()\n",
    "    merged_addis[f'{name}_Discharge'] = merged_addis.groupby(['MRN', 'Discharge_Date'])[f'{name}_Discharge'].ffill()\n",
    "    merged_addis.drop_duplicates(inplace=True)\n",
    "    merged_addis.loc[:, 'Discharge_Date'] = pd.to_datetime(merged_addis['Discharge_Date'])\n",
    "    \n",
    "    merged_suppdf = pd.merge(merged_addis, result_df, on=[\"MRN\", \"Discharge_Date\"], how=\"outer\")\n",
    "\n",
    "\n",
    "    float_rows_mask = (  \n",
    "        merged_suppdf['Test Result'].isna()    \n",
    "    )\n",
    "    \n",
    "    merged_suppdf_noNATs = merged_suppdf[~float_rows_mask]\n",
    "\n",
    "    # All test result entries with their dates\n",
    "    merged_suppdf_noNATs['Combined'] = merged_suppdf_noNATs.apply(lambda row: list(zip(row['RequestDate'], row['Test Result'])), axis=1)\n",
    "\n",
    "    # Remove multiple entries - a col containing all results per episode\n",
    "    merged_suppdf_noNATs['UniqueCombined'] = merged_suppdf_noNATs['Combined'].apply(lambda x: list(set(x)))\n",
    "    merged_suppdf_noNATs['UniqueCombined'] = merged_suppdf_noNATs['UniqueCombined'].apply(lambda x: [list(i) for i in set(map(tuple, x))])\n",
    "    merged_suppdf_noNATs['UniqueCombined'] = merged_suppdf_noNATs['UniqueCombined'].apply(lambda x: sorted(x, key=lambda pair: pair[0]))\n",
    "\n",
    "                   \n",
    "    num_pairs = max(len(pair_list) for pair_list in merged_suppdf_noNATs['UniqueCombined'])\n",
    "\n",
    "    # Have one column per result and per test date\n",
    "    for i in range(1, num_pairs + 1):\n",
    "        merged_suppdf_noNATs[f'{name}_DT {i}'] = merged_suppdf_noNATs['UniqueCombined'].apply(lambda x: x[i-1][0] if len(x) >= i else None)\n",
    "        merged_suppdf_noNATs[f'{name}_Result {i}'] = merged_suppdf_noNATs['UniqueCombined'].apply(lambda x: x[i-1][1] if len(x) >= i else None)\n",
    "\n",
    "    merged_suppdf_noNATs.drop(columns=['RequestDate', 'Test Result', 'Combined',], inplace=True)\n",
    "    merged_suppdf_noNATs.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Check admission and discharge results are correst\n",
    "    merged_suppdf_result1 = merged_suppdf_noNATs[(merged_suppdf_noNATs[f'{name}_Admission'].isnull()) | (merged_suppdf_noNATs[f'{name}_Result 1'] == merged_suppdf_noNATs[f'{name}_Admission'])]\n",
    "\n",
    "    filtered_rows = []\n",
    "    for index, row in merged_suppdf_result1.iterrows():\n",
    "        last_non_null = row.dropna().iloc[-1]  # Find the last non-null value in the row, check if this is our value upon discharge - if not, no test was done upon discharge\n",
    "        if pd.isna(row[f'{name}_Discharge']) or row[f'{name}_Discharge'] == last_non_null:\n",
    "            filtered_rows.append(index)\n",
    "\n",
    "    merged_suppdf_checked = merged_suppdf_result1.loc[filtered_rows]\n",
    "\n",
    "    merged_suppdf_checked[f'{name}_Admission'] = merged_suppdf_checked.groupby(['MRN', 'Discharge_Date'])[f'{name}_Admission'].bfill()\n",
    "    merged_suppdf_checked[f'{name}_Admission'] = merged_suppdf_checked.groupby(['MRN', 'Discharge_Date'])[f'{name}_Admission'].ffill()\n",
    "\n",
    "    merged_suppdf_checked[f'{name}_Discharge'] = merged_suppdf_checked.groupby(['MRN', 'Discharge_Date'])[f'{name}_Discharge'].bfill()\n",
    "    merged_suppdf_checked[f'{name}_Discharge'] = merged_suppdf_checked.groupby(['MRN', 'Discharge_Date'])[f'{name}_Discharge'].ffill()\n",
    "    suppdf_checkadmit = merged_suppdf_checked[(merged_suppdf_checked[f'{name}_Admission'].isnull()) | (merged_suppdf_checked[f'{name}_Result 1'] == merged_suppdf_checked[f'{name}_Admission'])]\n",
    "    suppdf_checkadmit.drop_duplicates(inplace=True)\n",
    "    finalsupp = pd.merge(suppdf_checkadmit, merged_addis, how='outer')\n",
    "    finalsupp.drop_duplicates(inplace=True)\n",
    "\n",
    "    ds.loc[:, 'Discharge_Date'] = pd.to_datetime(ds['Discharge_Date'])\n",
    "\n",
    "    # Merge all the suplementary data back with the origional dataset now that it is in the format that we want\n",
    "    finalsupp.loc[:, 'Discharge_Date'] = pd.to_datetime(finalsupp['Discharge_Date'])\n",
    "        \n",
    "    finalsupp[f'{name}_Admission'] = finalsupp.groupby(['MRN', 'Discharge_Date'])[f'{name}_Admission'].bfill()\n",
    "    finalsupp[f'{name}_Admission'] = finalsupp.groupby(['MRN', 'Discharge_Date'])[f'{name}_Admission'].ffill()\n",
    "\n",
    "    finalsupp[f'{name}_Discharge'] = finalsupp.groupby(['MRN', 'Discharge_Date'])[f'{name}_Discharge'].bfill()\n",
    "    finalsupp[f'{name}_Discharge'] = finalsupp.groupby(['MRN', 'Discharge_Date'])[f'{name}_Discharge'].ffill()\n",
    "    \n",
    "    ds_supp = pd.merge(ds, finalsupp, on=['MRN', 'Discharge_Date'], how='right')\n",
    "    ds_supp.sort_values(by=['MRN', 'Discharge_Date'], inplace=True)\n",
    "\n",
    "    ds_supp = ds_supp.loc[:, ~ds_supp.columns.str.contains('^Unnamed')]  \n",
    "    \n",
    "    # Fill the common columns - will have to ensure same naming first\n",
    "    common_columns = set(ds.columns).intersection(finalsupp.columns)\n",
    "    cols_to_ignore = ['MRN', 'Discharge_Date', 'Admission_Date']\n",
    "    cols_to_fill = common_columns - set(cols_to_ignore)\n",
    "\n",
    "    for col in cols_to_fill:\n",
    "        ds_supp[col + \"_x\"].fillna(ds_supp[col + \"_y\"], inplace=True)\n",
    "    \n",
    "    ds_supp.drop(columns=[col + '_y' for col in cols_to_fill], inplace=True)\n",
    "    ds_supp.columns = [col.replace('_x', '') for col in ds_supp.columns]\n",
    "\n",
    "    ds_supp.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Put date col in terms of days from admission rather than absolute  date\n",
    "    date_columns = [col for col in ds_supp.columns if col.startswith(f'{name}_DT')]\n",
    "    ds_supp[date_columns] = ds_supp[date_columns].apply(pd.to_datetime)\n",
    "    ds_supp['Admission_Date'] = ds_supp['Admission_Date'].apply(pd.to_datetime)\n",
    "    for col in date_columns:\n",
    "        ds_supp[col] = (ds_supp[col] - ds_supp['Admission_Date']).dt.days\n",
    "    \n",
    "    return ds_supp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db36445-8a3b-4342-a89d-329c20d7445d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CDS, Healtheintent, NICOR datasets\n",
    "CDS_raw = pd.read_csv(\"../01_Raw/HF_CDS.csv\", low_memory=False)\n",
    "Healtheintent_raw = pd.read_csv(\"../01_Raw/HF_Healtheintent.csv\", low_memory=False)\n",
    "NICOR_raw = pd.read_csv(\"../01_Raw/NICOR_NHFA_collated.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f33a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supplementary info\n",
    "raw_NT = pd.read_excel(\"../01_Raw/NTProBNP Pathology Nov19 to Date.xlsx\")\n",
    "weight = pd.read_csv(\"../01_Raw/weight_clean.csv\")\n",
    "hba1c = pd.read_csv(\"../01_Raw/hba1c_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745bedc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean NT-pro and prepare for merge\n",
    "NT_clean = raw_NT.dropna(subset=['PatientNumber', 'Test Result'], how='any')\n",
    "NT_clean = NT_clean.drop(['OBRExamCodeText', 'OBRExamCodeText.1', 'ObservationDate', 'ReportedDate', 'RequestingClinician', 'EncounterLocationType', 'ResultRange', 'ResultUnits', 'SEX'], axis='columns')\n",
    "NT_clean = NT_clean[NT_clean['Test Result'].apply(lambda x: bool(re.match(r'^([<>]\\d+|[\\d]+)$', str(x))))]\n",
    "NT_clean['RequestDate'] = pd.to_datetime(NT_clean['RequestDate'], format='%Y-%m-%d %H:%M:%S').dt.date\n",
    "NT_clean = NT_clean.rename(columns={'PatientNumber': 'MRN'})\n",
    "NT_clean.sort_values(by=['MRN', 'RequestDate'], ascending=False)\n",
    "NT_clean['MRN'] = NT_clean['MRN'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafa66d-e18b-4b0c-abf5-8b31b8b3051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean routine data and prepare for merge -- need consistent column names\n",
    "# CDS\n",
    "CDS_raw[\"Admission_Date\"] = pd.to_datetime(CDS_raw[\"Admission Date\"], format='%d/%m/%Y %H:%M').dt.date\n",
    "CDS_raw[\"Discharge_Date\"] = pd.to_datetime(CDS_raw[\"Discharge Date\"], format='%d/%m/%Y %H:%M').dt.date\n",
    "CDS_raw.rename(columns={'Died_In_Hospital': 'Died_while_Inpatient',\n",
    "                    'Died_Within30days_OfDischarge': 'Died_within_30days_of_Discharge',\n",
    "                    'Rockwood_WithinSpell_Score': 'Frailty_Score',\n",
    "                       'AdmissionMethod': 'Admission_Method'}, inplace=True)\n",
    "CDS_raw.dropna(subset=['MRN'], inplace=True)\n",
    "CDS_raw['MRN'] = CDS_raw['MRN'].astype(str)\n",
    "\n",
    "# Healtheintent\n",
    "Healtheintent_raw['Admission_Date'] = pd.to_datetime(Healtheintent_raw['Bm_Registration_Date'], format='%Y-%m-%d %H:%M:%S.%f').dt.date\n",
    "Healtheintent_raw['Discharge_Date'] = pd.to_datetime(Healtheintent_raw['Oa_Discharge_Date'], format='%Y-%m-%d %H:%M:%S.%f').dt.date\n",
    "Healtheintent_raw['DoB'] = pd.to_datetime(Healtheintent_raw['DoB'], format='%Y-%m-%d').dt.date\n",
    "Healtheintent_raw['Admission_Date'] = pd.to_datetime(Healtheintent_raw['Admission_Date'])\n",
    "Healtheintent_raw['DoB'] = pd.to_datetime(Healtheintent_raw['DoB'])\n",
    "Healtheintent_raw = Healtheintent_raw.dropna(subset=['DoB'])\n",
    "Healtheintent_raw['Age_on_Admission_calc'] = (Healtheintent_raw['Admission_Date'] - Healtheintent_raw['DoB']) // pd.Timedelta(days=365.25)\n",
    "Healtheintent_raw.rename(columns={'Aa_MRN': 'MRN',\n",
    "                                  'Ae_Deceased_Date': 'Deceased_Date',\n",
    "                                  'Af_Facility': 'Site',\n",
    "                                  'Ag_Ethnicity': 'Ethnicity',\n",
    "                                  'Ah_Patient_Age': 'Age_on_Admission',\n",
    "                                  'Ai_Gender': 'Gender',\n",
    "                                  'Aj_Died_while_Inpatient': 'Died_while_Inpatient',\n",
    "                                  'Al_Died_within_30days_of_Discharge': 'Died_within_30days_of_Discharge',\n",
    "                                  'Ba_Admission_Method': 'Admission_Method',\n",
    "                                  'Bf_7days_Readmission_': 'Readmission7d',\n",
    "                                  'Bg_14days_Readmission_': 'Readmission14d',\n",
    "                                  'Bh_30days_Readmission_': 'Readmission30d',\n",
    "                                  'Bi_180days_Readmission_': 'Readmission180d',\n",
    "                                  'Ga_Diagnosis_Confirmed': 'HF_Diagnosis_Confirmed',\n",
    "                                  'Ia_Frailty_Score_Values': 'Frailty_Score',\n",
    "                                  'Ja_Palliative_Care_Referral_': 'Palliative_Care_Referral',\n",
    "                                  'Ka_Cardiac_Rehabilitation_Referral_': 'Cardiac_Rehabilitation_Referral'}, inplace=True)\n",
    "Healtheintent_raw.dropna(subset=['MRN'], inplace=True)\n",
    "Healtheintent_raw['MRN'] = Healtheintent_raw['MRN'].astype(str)\n",
    "\n",
    "# NICOR\n",
    "NICOR_raw['Admission_Date'] = pd.to_datetime(NICOR_raw['2.00 Date of Admission'], format='%d/%m/%Y').dt.date\n",
    "NICOR_raw['Discharge_Date'] = pd.to_datetime(NICOR_raw['15.10 Date of discharge or death'], format='%d/%m/%Y').dt.date\n",
    "NICOR_raw.rename(columns={'1.02 Local Patient Identifier': 'MRN',\n",
    "                          '1.08 Ethnic Category': 'Ethnicity', \n",
    "                          'Age on Admission': 'Age_on_Admission',\n",
    "                          '1.07 Patient sex': 'Gender',\n",
    "                          '15.15 Death in hospital': 'Died_while_Inpatient',\n",
    "                          '14.00 Confirmed diagnosis of heart failure': 'HF_Diagnosis_Confirmed',\n",
    "                          '15.05 Was a referral to palliative care services made?': 'Palliative_Care_Referral',\n",
    "                          '15.01 Was a referral to cardiac rehabilitation made?': 'Cardiac_Rehabilitation_Referral',\n",
    "                          '1.01 Hospital identifier': 'Site',\n",
    "                          '8.02 Weight (discharge)': 'Weight_Discharge',\n",
    "                          '8.02a Weight (admission)': 'Weight_Admission'}, inplace=True)\n",
    "NICOR_raw.dropna(subset=['MRN'], inplace=True)\n",
    "NICOR_raw['MRN'] = NICOR_raw['MRN'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6530b645-b74d-41ed-a517-9e594300a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Healtheintent with CDS\n",
    "merged_df1 = merge_datasets(Healtheintent_raw, CDS_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ed2e1-fc24-4fab-bc6a-2d9608eb4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All three databases\n",
    "merged_df2 = merge_datasets(merged_df1, NICOR_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad84069-4d36-4ed3-84ae-a8d74ad5a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender, ethnicity, deceased status doesn't change by episdoe\n",
    "merged_df2['Ethnicity'] = merged_df2.groupby('MRN')['Ethnicity'].ffill().bfill()\n",
    "merged_df2['Gender'] = merged_df2.groupby('MRN')['Gender'].ffill().bfill()\n",
    "merged_df2['Ad_Current_Deceased_Status'] = merged_df2['Deceased_Date'].apply(lambda x: 'Deceased' if pd.notnull(x) else np.nan)\n",
    "merged_df2['Ad_Current_Deceased_Status'] = merged_df2.groupby('MRN')['Ad_Current_Deceased_Status'].bfill().ffill()\n",
    "merged_df2['Deceased_Date'] = merged_df2.groupby('MRN')['Deceased_Date'].bfill().ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a3915-335d-4f9d-bd39-55263471695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add NT-pro data to merged datasets\n",
    "NT_clean['MRN'] = NT_clean['MRN'].astype(str)\n",
    "df_NTpro = merge_suppdata(merged_df2, NT_clean, 'NTproBNP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ccf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Removing any with negative dates for NTproDT as issues with admission and discharge dates\n",
    "df_NTpro_corrected = df_NTpro[df_NTpro['NTproBNP_DT 1'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b26af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_NTpro_corrected.to_csv(\"../03_datasets/df_NTpro_corrected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b333207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean weight data\n",
    "weight['RequestDate'] = pd.to_datetime(weight['PERFORMED_DT_TM'], format='%d/%m/%Y %H:%M').dt.date\n",
    "weight.rename(columns={'Aa_MRN': 'MRN',\n",
    "                      'RESULT_VAL': 'Test Result'}, inplace=True)\n",
    "weight['MRN'] = weight['MRN'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c74045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains 3 datasets, NTpro, weight\n",
    "df_weight = merge_suppdata(df_NTpro_corrected, weight, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a91908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean HBA1c data\n",
    "hba1c['RequestDate'] = pd.to_datetime(hba1c['RequestDate'], format='%Y-%m-%d %H:%M:%S').dt.date\n",
    "hba1c.rename(columns={'PatientNumber': 'MRN'}, inplace=True)\n",
    "hba1c['MRN'] = hba1c['MRN'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains 3 datasets, NTpro, weight, hba1c\n",
    "df_hba1c = merge_suppdata(df_weight, hba1c, 'hba1c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fded7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hba1c.to_csv(\"../03_datasets/alldata.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
